<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Membership Inference Attack with Cumulative Topological Distance | Min Chun Chen</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from membership inference attacks, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality.
This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.">
<meta name="author" content="">
<link rel="canonical" href="https://www.daedluz.com/posts/membership-inference-attack-with-cumulative-topological-distance/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.daedluz.com/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.daedluz.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.daedluz.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.daedluz.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.daedluz.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.daedluz.com/posts/membership-inference-attack-with-cumulative-topological-distance/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://www.daedluz.com/posts/membership-inference-attack-with-cumulative-topological-distance/">
  <meta property="og:site_name" content="Min Chun Chen">
  <meta property="og:title" content="Membership Inference Attack with Cumulative Topological Distance">
  <meta property="og:description" content="Introduction As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from membership inference attacks, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality. This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-08-12T01:16:16-04:00">
    <meta property="article:modified_time" content="2025-08-12T01:16:16-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Membership Inference Attack with Cumulative Topological Distance">
<meta name="twitter:description" content="Introduction
As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from membership inference attacks, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality.
This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.daedluz.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Membership Inference Attack with Cumulative Topological Distance",
      "item": "https://www.daedluz.com/posts/membership-inference-attack-with-cumulative-topological-distance/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Membership Inference Attack with Cumulative Topological Distance",
  "name": "Membership Inference Attack with Cumulative Topological Distance",
  "description": "Introduction As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from membership inference attacks, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality. This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from membership inference attacks, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality. This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.\nMethodology Preliminaries In this section, I’ll briefly explain how membership inference attacks and TED-LaST works\nMembership Inference Attacks For a target model, the attacker would train a shadow model on a auxiliary dataset that belongs to the same data distribution as the target model’s training set.\nThen attacker would obtain the logits for member and non-member data of the shadow model. After that, the attacker would construct an attack dataset where $x$ is the logits obtained, and $y$ is the membership status.\nA attack model would then be trained on the attack dataset to distinguish between the logits of member and non-member samples.\nTED-LaST This is a method designed to detect backdoor samples in poisoning attacks, we will focus on the part of cumulative topological distance(ctd).\nFirst, it assumes that the defender has access to a set of clean sample (without trigger and correctly labeled). For each sample in the clean subset, obtain the activations of from each layer, then construct a K-nearest neighbor model for each layer.\nNext, for a sample $x$ that we want to calculate ctd, we first obtain its activation from each layer, denoted as $x_l$. We’ll then get k nearest neighbor for $x_l$ and calculate the rank of it’s nearest neighbor from the same class as $K_l$.\nBelow is as example: We take $x_l$ as an example, we iterate through its k nearest neighbors $n_{1…k}$, see if $n_i$ share the same label as $x$, the rank of that neighbor is $i-1$. After we find the nearest neighbor, we record it’s rank as $K_l$.\nUltimately, we will have a vector $K$. $K$ represents the topological evolution of a sample when it makes its way through the neural network.\nThe author also stated that each layer should be assigned different weights, but we will not cover the detail of weight calculation in this post.\nOverview After reading TED-LaST, I developed the idea of using CTD as a membership inference attack feature. My original hypothesis is if a model is trained on an image, it would recognize it and thus having a smaller CTD score compared to those the model are not trained on.\nSo basically the idea is to replace logits in traditional membership inference attacks into ctd values.\nFor the attack model, we considered two scenarios:\nTrain a pca-based outlier detector on the ctd values for non-members, then classify the outliers as members Train a multi-layer perceptron to distinguish between member and non-members Note that based on the nature of ctd, our attack needs to obtain activations from each layer of the neural network, making it a white-box attack. So we would expect a higher performance due to the extra information given.\nExperiment Experiment Setup I used ResNet 10 for both target and shadow model. The dataset used in this experiment is CIFAR-10, there’s 60000 images in this dataset, which is subsequently being divided into:\nTarget model training (25000) Target model validation (5000) Shadow model training (15000) Shadow model testing (10000) KNN samples (5000) Result Outlier Detector MLP Well, as we can see in the ROC curve, this attack is about the same as random guessing…\nDiscussion TED-LaST is originally designed for detecting poisoning attacks. The clean subset that are used to construct KNN act as ground truth answers that are not poisoned. Then we can view cumulative topological distance as a value representing “how misclassified an image is”. In TED-LaST, this value is calculated for each layer in the neural network.\nNext, we talk a little bit about adaptive attacks that TED-LaST tried to tackle with. In adaptive attacks, the attacker uses a technique called “laundry”, where the attack produces samples that contains the trigger but with correct label. This technique forces the model to associate the trigger with both the target class and its original class, making the trigger a “feature” the model considers when classifying an image. This trait makes the model prone to misclassifying the image when the image goes through the model, primarily between target label and the image’s original label. Since both classes contain this trait (poisoned samples and regularization samples).\nHowever, in membership inference attacks, the images does not contain triggers.\nSure there might be images that share similar traits (like cats and dogs both could have pointed ears), but this trait is consistent across all samples from the same class, whereas for backdoor detection, the trigger is only present in a subset of images, making that specific proportion of images more prone to misclassification as the sample goes through the model, resulting in a higher ctd values.\nIn the case of regular classification, all samples from the same class would have a similar cumulative topological distance, no matter if it’s a member or non-member, making this method ineffective on membership inference attacks.\n",
  "wordCount" : "884",
  "inLanguage": "en",
  "datePublished": "2025-08-12T01:16:16-04:00",
  "dateModified": "2025-08-12T01:16:16-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.daedluz.com/posts/membership-inference-attack-with-cumulative-topological-distance/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Min Chun Chen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.daedluz.com/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.daedluz.com/" accesskey="h" title="Min Chun Chen (Alt + H)">Min Chun Chen</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Membership Inference Attack with Cumulative Topological Distance
    </h1>
    <div class="post-meta"><span title='2025-08-12 01:16:16 -0400 EDT'>August 12, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>As machine learning models become increasingly powerful and pervasive, concerns about privacy in their deployment have grown. One notable risk comes from <strong>membership inference attacks</strong>, where an adversary attempts to determine whether a particular data point was used to train a given model. For example, in healthcare, a membership inference attack could reveal if someone participated in a clinical study, threatening patient confidentiality.
This risk motivates my work in this post, where I document my recent experiment investigating the possibility in using cumulative topological distance in membership inference attacks.</p>
<h1 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h1>
<h2 id="preliminaries">Preliminaries<a hidden class="anchor" aria-hidden="true" href="#preliminaries">#</a></h2>
<p>In this section, I&rsquo;ll briefly explain how membership inference attacks and TED-LaST works</p>
<h3 id="membership-inference-attacks">Membership Inference Attacks<a hidden class="anchor" aria-hidden="true" href="#membership-inference-attacks">#</a></h3>
<p>For a target model, the attacker would train a <em>shadow model</em> on a auxiliary dataset that belongs to the same data distribution as the target model&rsquo;s training set.</p>
<p>Then attacker would obtain the logits for member and non-member data of the shadow model. After that, the attacker would construct an <em>attack dataset</em> where $x$ is the logits obtained, and $y$ is the membership status.</p>
<p>A <em>attack model</em> would then be trained on the attack dataset to distinguish between the logits of member and non-member samples.</p>
<h3 id="ted-last">TED-LaST<a hidden class="anchor" aria-hidden="true" href="#ted-last">#</a></h3>
<p>This is a method designed to detect backdoor samples in poisoning attacks, we will focus on the part of cumulative topological distance(ctd).</p>
<p>First, it assumes that the defender has access to a set of clean sample (without trigger and correctly labeled).
For each sample in the clean subset, obtain the activations of from each layer, then construct a K-nearest neighbor model for each layer.</p>
<p>Next, for a sample $x$ that we want to calculate ctd, we first obtain its activation from each layer, denoted as $x_l$. We&rsquo;ll then get k nearest neighbor for $x_l$ and calculate the rank of it&rsquo;s nearest neighbor from the same class as $K_l$.</p>
<p>Below is as example:
<img alt="400" loading="lazy" src="https://i.imgur.com/3PuCmdi.jpeg"></p>
<p>We take $x_l$ as an example, we iterate through its k nearest neighbors $n_{1&hellip;k}$, see if $n_i$ share the same label as $x$, the rank of that neighbor is $i-1$. After we find the nearest neighbor, we record it&rsquo;s rank as $K_l$.</p>
<p>Ultimately, we will have a vector $K$. $K$ represents the topological evolution of a sample when it makes its way through the neural network.</p>
<p>The author also stated that each layer should be assigned different weights, but we will not cover the detail of weight calculation in this post.</p>
<h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>After reading TED-LaST, I developed the idea of using CTD as a membership inference attack feature. My original hypothesis is if a model is trained on an image, it would recognize it and thus having a smaller CTD score compared to those the model are not trained on.</p>
<p>So basically the idea is to replace logits in traditional membership inference attacks into ctd values.</p>
<p>For the attack model, we considered two scenarios:</p>
<ul>
<li>Train a pca-based outlier detector on the ctd values for non-members, then classify the outliers as members</li>
<li>Train a multi-layer perceptron to distinguish between member and non-members</li>
</ul>
<p>Note that based on the nature of ctd, our attack needs to obtain activations from each layer of the neural network, making it a white-box attack. So we would expect a higher performance due to the extra information given.</p>
<h1 id="experiment">Experiment<a hidden class="anchor" aria-hidden="true" href="#experiment">#</a></h1>
<h2 id="experiment-setup">Experiment Setup<a hidden class="anchor" aria-hidden="true" href="#experiment-setup">#</a></h2>
<p>I used ResNet 10 for both target and shadow model.
The dataset used in this experiment is <strong>CIFAR-10</strong>, there&rsquo;s 60000 images in this dataset, which is subsequently being divided into:</p>
<ul>
<li>Target model training (25000)</li>
<li>Target model validation (5000)</li>
<li>Shadow model training (15000)</li>
<li>Shadow model testing (10000)</li>
<li>KNN samples (5000)</li>
</ul>
<h2 id="result">Result<a hidden class="anchor" aria-hidden="true" href="#result">#</a></h2>
<h6 id="outlier-detector">Outlier Detector<a hidden class="anchor" aria-hidden="true" href="#outlier-detector">#</a></h6>
<p><img alt="500" loading="lazy" src="https://i.imgur.com/7VhZkrs.png"></p>
<h6 id="mlp">MLP<a hidden class="anchor" aria-hidden="true" href="#mlp">#</a></h6>
<p><img alt="500" loading="lazy" src="https://i.imgur.com/f21wnvO.png"></p>
<p>Well, as we can see in the ROC curve, this attack is about the same as random guessing&hellip;</p>
<h1 id="discussion">Discussion<a hidden class="anchor" aria-hidden="true" href="#discussion">#</a></h1>
<p>TED-LaST is originally designed for detecting poisoning attacks. The clean subset that are used to construct KNN act as ground truth answers that are not poisoned. Then we can view cumulative topological distance as a value representing &ldquo;how misclassified an image is&rdquo;. In TED-LaST, this value is calculated for each layer in the neural network.</p>
<p>Next, we talk a little bit about adaptive attacks that TED-LaST tried to tackle with. In adaptive attacks, the attacker uses a technique called &ldquo;laundry&rdquo;, where the attack produces samples that <strong>contains the trigger but with correct label</strong>. This technique forces the model to associate the trigger with both the target class and its original class, making the trigger a &ldquo;feature&rdquo; the model considers when classifying an image. This trait makes the model prone to misclassifying the image when the image goes through the model, primarily between target label and the image&rsquo;s original label. Since both classes contain this trait (poisoned samples and regularization samples).</p>
<p>However, in membership inference attacks, the images does not contain triggers.</p>
<p>Sure there might be images that share similar traits (like cats and dogs both could have pointed ears), but this trait is consistent across all samples from the same class, whereas for backdoor detection, the trigger is only present in a <strong>subset of images</strong>, making that specific proportion of images more prone to misclassification as the sample goes through the model, resulting in a higher ctd values.</p>
<p>In the case of regular classification, all samples from the same class would have a similar cumulative topological distance, no matter if it&rsquo;s a member or non-member, making this method ineffective on membership inference attacks.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://www.daedluz.com/">Min Chun Chen</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
